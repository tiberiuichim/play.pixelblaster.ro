<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machinelearning on The Plone Expanse</title>
    <link>/tags/machinelearning/index.xml</link>
    <description>Recent content in Machinelearning on The Plone Expanse</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>@2016 Tiberiu Ichim</copyright>
    <atom:link href="/tags/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Another way to index category labels in categorization tasks</title>
      <link>/blog/2017/01/21/another-way-to-index-category-labels-in-categorization-tasks/</link>
      <pubDate>Sat, 21 Jan 2017 00:19:31 +0100</pubDate>
      
      <guid>/blog/2017/01/21/another-way-to-index-category-labels-in-categorization-tasks/</guid>
      <description>&lt;p&gt;One common task in machine-based categorization tasks is to relabel data with
a numeric value, an index, where before that data was labeled with a string.&lt;/p&gt;

&lt;p&gt;The basic Python code would be something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;label_index = {}
labels = []
for l in string_labels:
    if l not in label_index:
        label_index[l] = len(label_index)
    labels.append(label_index[l])&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While writing that bit of code from above, I realized that a word tokenizer can
do the same thing. This would be the equivalent, using
&lt;code&gt;keras.preprocessing.text.Tokenizer&lt;/code&gt; and numpy.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;t = Tokenizer()
t.fit_on_texts(_labels)
seqs = t.texts_to_sequences(_labels)
# seqs is a list of lists, something like:
# [[1], [2], [1], [3] ... ]
labels = np.array(seqs)[:,0] - 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now &lt;code&gt;t.word_index&lt;/code&gt; holds the word to index mapping. &lt;code&gt;np.array(seqs)[:,0]&lt;/code&gt;
returns a list with the first element (at index 0) of the second dimension of
the np array. The arithmetic operation of the end applies to each member of
the array and is needed because the tokenizer starts indexing words at 1.&lt;/p&gt;

&lt;p&gt;Of course, the second way is convoluted, needs to process the list of labels
twice, uses two addon libraries (but probably already present for the domain
of this task) and needs some basic knowledge of numpy to be readable by others.
YMMV. I wonder which method is faster, on a bigger chunk of data.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>